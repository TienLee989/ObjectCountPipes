# ğŸ— **Topology cá»§a Há»‡ thá»‘ng PhÃ¡t hiá»‡n vÃ  Äáº¿m Äá»‘i tÆ°á»£ng**  

##### Link model training
[Model yolo after 700 epochs and enhance image](https://drive.google.com/file/d/1zwTjVbIii7afvw-E-_x_N18gXeMUzjfq/view?usp=drive_link)

[Attention Unet5](https://drive.google.com/file/d/1Sg7u8kZa7zPE3XgtB3xg-Ed3MitKNlBr/view?usp=drive_link)

[Nhá»¯ng hÃ¬nh áº£nh Ä‘áº§u ra cáº£u quÃ¡ trÃ¬nh test](https://drive.google.com/drive/folders/1cb-z6BdstMr9oSzAnVAikZuJ4JVnyvZ9?usp=drive_link)

## ğŸ“ **1. ThÃ nh pháº§n chÃ­nh**  
- **Input**: áº¢nh tá»« cÃ¡c thÆ° má»¥c theo loáº¡i Ä‘á»‘i tÆ°á»£ng (type folder).  
- **Models**:  
  - **YOLOv11m2**: PhÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng trong áº£nh.  
  - **Attention U-Net**: XÃ¡c Ä‘á»‹nh vÃ¹ng quan tÃ¢m (ROI) chá»©a Ä‘á»‘i tÆ°á»£ng.  
- **Clustering**: Sá»­ dá»¥ng **DBSCAN** Ä‘á»ƒ xá»­ lÃ½ tÃ¬nh huá»‘ng cÃ³ nhiá»u Ä‘á»‘i tÆ°á»£ng gáº§n nhau.  

---

## âš™ï¸ **2. Quy trÃ¬nh hoáº¡t Ä‘á»™ng**  

```mermaid
flowchart TD
    A[Input Image] --> B[Attention U-Net]
    B --> C[Dá»± Ä‘oÃ¡n ROI]
    C --> D{ROI TÃ¬m tháº¥y?}
    
    D -->|KhÃ´ng| E[Äáº¿m báº±ng UNet - Connected Components]
    D -->|CÃ³| F[Crop Image theo ROI]
    
    F --> G[YOLOv11m2 Detection]
    G --> H[NMS - Loáº¡i bá» trÃ¹ng láº·p]
    H --> I{Äá»‘i tÆ°á»£ng gáº§n nhau?}
    
    I -->|CÃ³| J[DBSCAN Clustering]
    I -->|KhÃ´ng| K[Final YOLO Count]
    
    E --> L[Final UNet Count]
    J --> K
    K --> M[Káº¿t quáº£ cuá»‘i cÃ¹ng]
```

## ğŸ›  **3. CÃ¡c hÃ m chÃ­nh**

### `pipeline_inference_with_unet_roi(image_path)`
- Cháº¡y **U-Net** Ä‘á»ƒ dá»± Ä‘oÃ¡n vÃ¹ng chá»©a váº­t thá»ƒ.  
- Cháº¡y **YOLO** trÃªn vÃ¹ng **ROI** hoáº·c dÃ¹ng káº¿t quáº£ **U-Net** náº¿u **YOLO** khÃ´ng phÃ¡t hiá»‡n.  

### `apply_nms(boxes, iou_threshold)`
- Ãp dá»¥ng **Non-Max Suppression (NMS)** Ä‘á»ƒ loáº¡i bá» cÃ¡c **bbox** trÃ¹ng láº·p.  

### `adjust_for_crowded(predicted_boxes)`
- DÃ¹ng thuáº­t toÃ¡n **DBSCAN** gom nhÃ³m cÃ¡c Ä‘á»‘i tÆ°á»£ng gáº§n nhau.  

### `load_ground_truth_count(label_path, expected_class)`
- Äáº¿m sá»‘ lÆ°á»£ng **ground truth** trong tá»‡p nhÃ£n.  

### `evaluate_yolo_on_type_folder(base_folder)`
- Cháº¡y Ä‘Ã¡nh giÃ¡ trÃªn tá»«ng thÆ° má»¥c theo loáº¡i Ä‘á»‘i tÆ°á»£ng.  

---

## ğŸ· **4. Káº¿t quáº£ Äáº§u ra**  

- **YOLO Count**: Sá»‘ lÆ°á»£ng Ä‘áº¿m Ä‘Æ°á»£c tá»« **YOLO**.  
- **UNet Count**: Sá»‘ lÆ°á»£ng Ä‘áº¿m Ä‘Æ°á»£c tá»« **U-Net**.  
- **Final Count**: Káº¿t quáº£ cuá»‘i cÃ¹ng (Æ°u tiÃªn YOLO náº¿u cÃ³).  
- **ROI BBox**: VÃ¹ng bao cá»§a **ROI** trong áº£nh gá»‘c.  

---

## ğŸ“Š **5. CÃ´ng nghá»‡ vÃ  ThÆ° viá»‡n**  

- **YOLOv11m2**: MÃ´ hÃ¬nh phÃ¡t hiá»‡n Ä‘á»‘i tÆ°á»£ng.  
- **Attention U-Net**: PhÃ¢n Ä‘oáº¡n áº£nh vÃ  xÃ¡c Ä‘á»‹nh vÃ¹ng chá»©a Ä‘á»‘i tÆ°á»£ng.  
- **DBSCAN (Sklearn)**: Gom nhÃ³m Ä‘á»‘i tÆ°á»£ng.  
- **OpenCV**: Xá»­ lÃ½ áº£nh.  
- **Numpy, Torch**: Xá»­ lÃ½ sá»‘ liá»‡u vÃ  tensor.

## ğŸ— Topology cho mÃ´ hÃ¬nh Attention U-Net vÃ  quy trÃ¬nh huáº¥n luyá»‡n
Huáº¥n luyá»‡n mÃ´ hÃ¬nh Attention U-Net vá»›i Self-Attention cho bÃ i toÃ¡n phÃ¢n Ä‘oáº¡n áº£nh (Image Segmentation)

```mermaid
flowchart TD
  subgraph "Xu ly Dataset"
    A[Load Annotations from JSON file]
    B[Mapping image_id to file_name]
    C[Mapping file_name to annotations]
    D[Data Generator]
    E[Load Image from image_dir]
    F[Create Mask from bbox - Annotation]
    G[Resize Image and Mask to input_size]
    H[Output Batch: Images and Masks]
    A --> B
    B --> C
    C --> D
    D --> E
    E --> F
    F --> G
    G --> H
  end
```

```mermaid
flowchart TD
    subgraph "Xay dung Attention U-Net"
      I[Input Layer: input_shape]
      J[Encoder Block 1: Conv Block 64 + MaxPooling]
      K[Encoder Block 2: Conv Block 128 + MaxPooling]
      L[Encoder Block 3: Conv Block 256 + MaxPooling]
      M[Encoder Block 4: Conv Block 512 + MaxPooling]
      N[Encoder Block 5: Conv Block 1024]
      O[Self-Attention Layer: Bottleneck]
      P[Decoder Block 1: UpSampling + AttentionGate skip Block4 + Concatenate + Conv Block 512]
      Q[Decoder Block 2: UpSampling + AttentionGate skip Block3 + Concatenate + Conv Block 256]
      R[Decoder Block 3: UpSampling + AttentionGate skip Block2 + Concatenate + Conv Block 128]
      S[Decoder Block 4: UpSampling + AttentionGate skip Block1 + Concatenate + Conv Block 64]
      T[Output Layer: Conv2D with softmax]
      I --> J
      J --> K
      K --> L
      L --> M
      M --> N
      N --> O
      O --> P
      P --> Q
      Q --> R
      R --> S
      S --> T
    end

    subgraph "Huan luyen mo hinh"
      U[Train Generator: batch_size, epochs]
      V[Compile Model: optimizer Adam, loss sparse_categorical_crossentropy]
      W[Model.fit: Train, Validation, Callbacks]
      X[ModelCheckpoint Callback: Save best weights]
      Y[Save final weights: attention_unet_coco.h5]
      T --> U
      U --> V
      V --> W
      W --> X
      X --> Y
    end
```
